{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EitanBakirov/Economics-Data-Science/blob/main/Web_Scraping_and_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Web Scraping and APIs for Data Scientists"
      ],
      "metadata": {
        "id": "se1FJ0qiLbJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation"
      ],
      "metadata": {
        "id": "pB4KLLw6KlAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the world of data science, being able to get and use data from the web is very important. Two key methods that help data scientists do this are web scraping and APIs (Application Programming Interfaces). This notebook is a complete guide for beginners, showing how to gather useful data from websites and online services. Whether you need data for research, to build a dataset for machine learning, or to automate repetitive tasks, learning web scraping and APIs will be very helpful. With practical examples, pictures, and clear steps, this guide will teach you the basics and get you started in the data-rich world of the web."
      ],
      "metadata": {
        "id": "gCaY6oStJocf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview of Web Scraping and APIs\n",
        "\n",
        "- **Web Scraping**: Web scraping is a technique used to extract data from websites. It involves fetching the HTML of a webpage and then parsing it to find the necessary information. This is useful when there is no API available, or when the data you need is only displayed on a webpage.\n",
        "  - **Use Cases**: Gathering product information from e-commerce sites, collecting social media posts, extracting news articles, etc.\n",
        "  - **Common Tools**: BeautifulSoup, Scrapy, Selenium\n",
        "\n",
        "- **APIs**: An API is a set of rules and protocols that allows different software applications to communicate with each other. Many websites and services provide APIs that allow you to programmatically request and retrieve data.\n",
        "  - **Use Cases**: Accessing structured data from services like Spotify, Twitter (X), weather information, stock prices, etc.\n",
        "  - **Common Tools**: Requests library, Postman, various language-specific libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ukSmLDx7Lq37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance in Data Science\n",
        "\n",
        "Web scraping and APIs are essential skills for data scientists for several reasons:\n",
        "\n",
        "1. **Data Availability**: Much of the data needed for analysis, model building, or business intelligence is available on the web.\n",
        "2. **Automation**: These techniques allow for the automated collection of large datasets, saving time and effort compared to manual data collection.\n",
        "3. **Customization**: By using web scraping and APIs, you can tailor the data collection process to your specific needs, gathering only the information relevant to your project.\n",
        "4. **Integration**: Combining data from multiple sources (both scraped data and API data) can provide a more comprehensive dataset for deeper insights and more robust models.\n",
        "\n",
        "With this understanding, let's dive into setting up our environment and getting started with web scraping and APIs."
      ],
      "metadata": {
        "id": "EnjpNCfILtU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n"
      ],
      "metadata": {
        "id": "xPQh02s_Mw_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Your Environment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kcmQSBNaM0a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Necessary Libraries\n",
        "\n",
        "Before we dive into web scraping and APIs, we need to set up our environment by installing some essential libraries. These libraries will help us fetch and process data from the web.\n",
        "\n",
        "The main libraries we will use are:\n",
        "- **BeautifulSoup**: For parsing HTML and extracting data from web pages.\n",
        "- **Requests**: For making HTTP requests to web servers.\n",
        "- **Selenium**: For automating web browsers (useful for scraping dynamic content).\n",
        "- **Pandas**: For data manipulation and analysis.\n",
        "- **JSON**: For handling JSON data (commonly returned by APIs).\n",
        "\n",
        "Let's install these libraries. If you are using Google Colab, these can be installed using pip:"
      ],
      "metadata": {
        "id": "4tszvhNlM2FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary libraries\n",
        "!pip install beautifulsoup4 requests selenium pandas"
      ],
      "metadata": {
        "id": "0UwZkniKM7Wg",
        "outputId": "260fbc62-570c-4020-8c8a-d414c1d8f391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by verifying the installation of our libraries and exploring a simple example."
      ],
      "metadata": {
        "id": "q-afPdDOhlyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Verifying the installations\n",
        "print(\"Libraries installed and ready to use!\")\n"
      ],
      "metadata": {
        "id": "3LGDmf9jhm9M",
        "outputId": "18d8b09c-388e-4a32-840c-49fef6d2db76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next sections, we will explore web scraping in detail, starting with understanding HTML structure and how to navigate it using BeautifulSoup."
      ],
      "metadata": {
        "id": "2A0oP66ChujC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping\n",
        "\n"
      ],
      "metadata": {
        "id": "VdaHzQLLh-g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Web Scraping?\n",
        "\n",
        "Web scraping is a technique used to extract data from websites. It involves fetching the HTML of a webpage and then parsing it to find the necessary information. This is useful when there is no API available, or when the data you need is only displayed on a webpage.\n",
        "\n"
      ],
      "metadata": {
        "id": "fWHfsacriB9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ethical Considerations and Legal Aspects\n",
        "\n",
        "Before you start web scraping, it’s important to consider the ethical and legal implications:\n",
        "- **Respect Website Terms of Service**: Always check a website’s terms of service to ensure you are not violating any rules.\n",
        "- **Be Polite**: Avoid making too many requests in a short period. Use delays between requests to prevent overloading the server.\n",
        "- **Use an API if Available**: If a website provides an API, use it instead of scraping. APIs are designed for data access and are often more efficient and reliable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_s9AUtxiEZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Concepts\n",
        "\n"
      ],
      "metadata": {
        "id": "qFDH9kGSiLoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HTML Structure\n",
        "\n",
        "Web pages are structured using HTML (HyperText Markup Language). Understanding the basic structure of HTML is crucial for web scraping.\n",
        "\n",
        "Here's a simple example of an HTML structure:\n",
        "\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Welcome to the Sample Page</h1>\n",
        "    <p>This is a sample paragraph.</p>\n",
        "    <div class=\"content\">\n",
        "        <p>More content here.</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "```"
      ],
      "metadata": {
        "id": "b10wjNHoiPX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSS Selectors\n",
        "\n"
      ],
      "metadata": {
        "id": "Vl5wkeMDjJ_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSS (Cascading Style Sheets) selectors are used to select and style HTML elements. They are also useful for selecting elements when scraping.\n",
        "\n",
        "- Element Selector: p selects all <p> elements.\n",
        "- Class Selector: .content selects all elements with class \"content\".\n",
        "- ID Selector: #main selects the element with id \"main\"."
      ],
      "metadata": {
        "id": "m4Z9ZGPgmCpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools and Libraries for Web Scraping\n",
        "\n"
      ],
      "metadata": {
        "id": "v7r2vn_NkLV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### BeautifulSoup\n",
        "\n",
        "BeautifulSoup is a Python library used for parsing HTML and XML documents. It creates a parse tree that can be used to extract data from HTML.\n"
      ],
      "metadata": {
        "id": "qeLhe6rqkUZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Scrapy\n",
        "Scrapy is an open-source and collaborative web crawling framework for Python. It is used to extract data from websites and process them as required.\n",
        "\n"
      ],
      "metadata": {
        "id": "IG8-Z37mkaM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selenium\n",
        "Selenium is a powerful tool for controlling web browsers through programs and performing browser automation. It is useful for scraping dynamic content that requires JavaScript execution."
      ],
      "metadata": {
        "id": "KnqkvF5nkbKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Examples\n"
      ],
      "metadata": {
        "id": "ioY4meVbkk_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple HTML Parsing with BeautifulSoup\n",
        "Let's start with a basic example of using BeautifulSoup to parse HTML and extract data."
      ],
      "metadata": {
        "id": "fKYy6l2Fk2iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Fetch the webpage\n",
        "url = 'https://ollivere.co/'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the title of the page\n",
        "title = soup.title.string\n",
        "print('Page Title:', title)\n",
        "\n",
        "# Extract all paragraphs\n",
        "paragraphs = soup.find_all('p')\n",
        "for i, p in enumerate(paragraphs, 1):\n",
        "    print(f'Paragraph {i}:', p.text)\n"
      ],
      "metadata": {
        "id": "I2gC9zTFh-Av",
        "outputId": "1111183d-8ce0-490b-f18b-f9b422431ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Ollivere | Digital Product, UX and Branding Design Consultancy\n",
            "Paragraph 1: Creative Direction - how to make things different & memorable\n",
            "Paragraph 2: End-to-end UX and UI design of websites & apps.\n",
            "Paragraph 3: Workshops to clarify brand positioning and messaging.\n",
            "Paragraph 4: Branding and design kits/systems\n",
            "Paragraph 5: Bespoke illustration, animation & data visualisation.\n",
            "Paragraph 6: Design and build creative Webflow websites (like this one). \n",
            "Paragraph 7: Properly understand the project goals, who the audience are, what they want and what already exists.\n",
            "Paragraph 8: Test early & often using sketches, mock-ups & prototypes to quickly learn what works.\n",
            "Paragraph 9: Create organised systems of design & brand components that are easy to use & maintain.\n",
            "Paragraph 10: Continuously refine & develop over time based on methodically gathered feedback.\n",
            "Paragraph 11: “Martin makes my life easy. He’s one of those rare people that I can give a brief to and then relax, because I know he’ll come up with something fantastic.”\n",
            "Paragraph 12: “Working with Martin has been a pleasure - he is a true professional who always delivers what he promises!”\n",
            "Paragraph 13: \"Martin is an exceptional designer who goes above and beyond to understand and incorporate the client's vision and bring it to life in his design.\n",
            "Paragraph 14: Martin worked with us to deliver our responsible technology consumption app Goozby from concept to launch, including focus group testing, design concepts and UX. Goozby was finalist for the prestigious Meffys 'Safety in Consumer Life' award 2019, a testament to the level of quality and innovative thinking he applies. I would highly recommend Martin and look forward to working with him again in the future.\"\n",
            "Paragraph 15: \"We were building a new platform and brand that needed to look modern and cater to the nuanced needs of our customers, and Martin delivered.\n",
            "Paragraph 16: I was very impressed with his candid, no nonsense attitude and his UX and design expertise. It was a pleasure working with Martin and we look forward to an ongoing relationship that helps drive our products forward.\"\n",
            "Paragraph 17: Branding, illustrations and website for a healthcare start-up that helps people improve mental health in a live online setting with trained coaches\n",
            "Paragraph 18: New branding, design system and website for the world’s biggest product management school\n",
            "Paragraph 19: Branding, product UX/UI and Webflow website for a new auto-updating contact management app \n",
            "Paragraph 20: Web app that saves designers and developers much time and effort when creating typography designs. \n",
            "Paragraph 21: Branding and website for a Peter Thiel backed med-tech start-up that's developing a revolutionary treatment for sleep apnea.\n",
            "Paragraph 22: Branding for a habit and task tracking app designed to make life more more peaceful by nudging people towards positive behaviours.\n",
            "Paragraph 23: Mobile app using game-like challenges and positive nudges to encourage young people to develop healthier tech habits.\n",
            "Paragraph 24: Branding, website and illustrations for self-flying plane builder Ribbit, who help people operate their own pilotless airline\n",
            "Paragraph 25: Promotional websites for the acclaimed trilogy of novels, The Darkest Hand.\n",
            "Paragraph 26: Data viz showing what times of the year, month & week require more downloads to reach high app store chart positions.\n",
            "Paragraph 27: Modular dashboard that allows app developers to track all their disparate sales metrics in one place.\n",
            "Paragraph 28: A variety of charts for displaying different app performance data within a single modular dashboard.\n",
            "Paragraph 29: Branding for a specialist provider of occupational therapy services to kids.\n",
            "Paragraph 30: Design of a mobile horror game designed to test nerves and reaction speeds.\n",
            "Paragraph 31: Track commonly appearing words in an app's reviews to spot and solve any issues before they impact the average rating.\n",
            "Paragraph 32: See where in the world an app has potential to make the most money.\n",
            "Paragraph 33: Product design for a new and improved HR applicant tracking system that reaps big time savings through simple design.\n",
            "Paragraph 34: Branding and website for a CFO-on-demand service aimed at start-ups who need part-time help with their finances.\n",
            "Paragraph 35: Design and production of an interactive storybook for iPad. \n",
            "Paragraph 36: Branding, product design & website  for new analytics platform that makes it easier to track the performance of instagram posts.\n",
            "Paragraph 37: Branding, illustrations and website for app development course 'The Adventure of Inventing'\n",
            "Paragraph 38: Product design for ed-tech start-up helping educators with no coding experience teach kids to think like programmers.\n",
            "Paragraph 39: Branding and website for a new music industry data start-up that helps artists track royalties. Backed by some big industry names.\n",
            "Paragraph 40: Design of data viz showing who hogs the world's wealth. (Shortlisted for Kantar Information is Beautiful award)\n",
            "Paragraph 41: New Branding and website for an investment intelligence platform that gives non-technical investors the ability to use data-based techniques\n",
            "Paragraph 42: Branding and website for a new pediatric virtual therapy start-up that helps families and kids manage ADHD\n",
            "Paragraph 43: Product design for an app that lets designers and developers create CSS gradients in a new, visual way. \n",
            "Paragraph 44: Branding for a platform that helps new car buyers to gather competing quotes from dealers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Navigating and Scraping Complex Web Pages"
      ],
      "metadata": {
        "id": "-dg8H2balAQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more complex pages, we can use CSS selectors to find elements."
      ],
      "metadata": {
        "id": "xxcJ08vklCeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract elements with class 'content'\n",
        "content_divs = soup.select('.content')\n",
        "for div in content_divs:\n",
        "    print('Content:', div.text)"
      ],
      "metadata": {
        "id": "kAPeDB_4lEvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Dynamic Content with Selenium"
      ],
      "metadata": {
        "id": "7lVYBZRClKaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with dynamic content, Selenium can be used to render JavaScript."
      ],
      "metadata": {
        "id": "B970IHrDlNI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n",
        "# Set up the Selenium WebDriver (using Chrome in this example)\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Navigate to the webpage\n",
        "driver.get('http://example.com')\n",
        "\n",
        "# Allow time for JavaScript to execute\n",
        "time.sleep(5)\n",
        "\n",
        "# Extract data\n",
        "dynamic_content = driver.find_element(By.CLASS_NAME, 'dynamic')\n",
        "print('Dynamic Content:', dynamic_content.text)\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "iFkZ1MgOlMZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Storage"
      ],
      "metadata": {
        "id": "RHN_AsdKlV2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing and Cleaning Scraped Data\n"
      ],
      "metadata": {
        "id": "c5SXgXInlXXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After extracting data, you may need to clean it for analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "4s2oppgBlZEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of cleaning data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Paragraphs': [p.text for p in paragraphs]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "SEAU6U45lZ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing Data in CSV, JSON, or Databases\n"
      ],
      "metadata": {
        "id": "tlXQI4UXlbNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can store the cleaned data in various formats for later use.\n",
        "\n"
      ],
      "metadata": {
        "id": "EGcQ5R59ltXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "df.to_csv('scraped_data.csv', index=False)\n",
        "\n",
        "# Save to JSON\n",
        "df.to_json('scraped_data.json', orient='records')"
      ],
      "metadata": {
        "id": "xRbwMu_3luUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these examples and tools, you should be able to start scraping data from web pages. In the next section, we will explore APIs and how to use them to gather data."
      ],
      "metadata": {
        "id": "IK09-IGFlwD7"
      }
    }
  ]
}