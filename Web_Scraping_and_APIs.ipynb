{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EitanBakirov/Economics-Data-Science/blob/main/Web_Scraping_and_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Web Scraping and APIs for Data Scientists"
      ],
      "metadata": {
        "id": "se1FJ0qiLbJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation"
      ],
      "metadata": {
        "id": "pB4KLLw6KlAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the world of data science, being able to get and use data from the web is very important. Two key methods that help data scientists do this are web scraping and APIs (Application Programming Interfaces). This notebook is a complete guide for beginners, showing how to gather useful data from websites and online services. Whether you need data for research, to build a dataset for machine learning, or to automate repetitive tasks, learning web scraping and APIs will be very helpful. With practical examples, pictures, and clear steps, this guide will teach you the basics and get you started in the data-rich world of the web."
      ],
      "metadata": {
        "id": "gCaY6oStJocf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview of Web Scraping and APIs\n",
        "\n",
        "- **Web Scraping**: Web scraping is a technique used to extract data from websites. It involves fetching the HTML of a webpage and then parsing it to find the necessary information. This is useful when there is no API available, or when the data you need is only displayed on a webpage.\n",
        "  - **Use Cases**: Gathering product information from e-commerce sites, collecting social media posts, extracting news articles, etc.\n",
        "  - **Common Tools**: BeautifulSoup, Scrapy, Selenium\n",
        "\n",
        "- **APIs**: An API is a set of rules and protocols that allows different software applications to communicate with each other. Many websites and services provide APIs that allow you to programmatically request and retrieve data.\n",
        "  - **Use Cases**: Accessing structured data from services like Spotify, Twitter (X), weather information, stock prices, etc.\n",
        "  - **Common Tools**: Requests library, Postman, various language-specific libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ukSmLDx7Lq37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance in Data Science\n",
        "\n",
        "Web scraping and APIs are essential skills for data scientists for several reasons:\n",
        "\n",
        "1. **Data Availability**: Much of the data needed for analysis, model building, or business intelligence is available on the web.\n",
        "2. **Automation**: These techniques allow for the automated collection of large datasets, saving time and effort compared to manual data collection.\n",
        "3. **Customization**: By using web scraping and APIs, you can tailor the data collection process to your specific needs, gathering only the information relevant to your project.\n",
        "4. **Integration**: Combining data from multiple sources (both scraped data and API data) can provide a more comprehensive dataset for deeper insights and more robust models.\n",
        "\n",
        "With this understanding, let's dive into setting up our environment and getting started with web scraping and APIs."
      ],
      "metadata": {
        "id": "EnjpNCfILtU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n"
      ],
      "metadata": {
        "id": "xPQh02s_Mw_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Your Environment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kcmQSBNaM0a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO Maybe split the imports and libraries to API and Web Scraping specifically"
      ],
      "metadata": {
        "id": "wPXYBWi6yJH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Necessary Libraries\n",
        "\n",
        "Before we dive into web scraping and APIs, we need to set up our environment by installing some essential libraries. These libraries will help us fetch and process data from the web.\n",
        "\n",
        "The main libraries we will use are:\n",
        "- **BeautifulSoup**: For parsing HTML and extracting data from web pages.\n",
        "- **Requests**: For making HTTP requests to web servers.\n",
        "- **Selenium**: For automating web browsers (useful for scraping dynamic content).\n",
        "- **Pandas**: For data manipulation and analysis.\n",
        "- **JSON**: For handling JSON data (commonly returned by APIs).\n",
        "\n",
        "Let's install these libraries. If you are using Google Colab, these can be installed using pip:"
      ],
      "metadata": {
        "id": "4tszvhNlM2FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary libraries\n",
        "!pip install beautifulsoup4 requests selenium pandas"
      ],
      "metadata": {
        "id": "0UwZkniKM7Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260fbc62-570c-4020-8c8a-d414c1d8f391"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by verifying the installation of our libraries and exploring a simple example."
      ],
      "metadata": {
        "id": "q-afPdDOhlyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Verifying the installations\n",
        "print(\"Libraries installed and ready to use!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LGDmf9jhm9M",
        "outputId": "18d8b09c-388e-4a32-840c-49fef6d2db76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next sections, we will explore web scraping in detail, starting with understanding HTML structure and how to navigate it using BeautifulSoup."
      ],
      "metadata": {
        "id": "2A0oP66ChujC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping\n",
        "\n"
      ],
      "metadata": {
        "id": "VdaHzQLLh-g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Web Scraping?\n",
        "\n",
        "Web scraping is a technique used to extract data from websites. It involves fetching the HTML of a webpage and then parsing it to find the necessary information. This is useful when there is no API available, or when the data you need is only displayed on a webpage.\n",
        "\n"
      ],
      "metadata": {
        "id": "fWHfsacriB9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ethical Considerations and Legal Aspects\n",
        "\n",
        "Before you start web scraping, it’s important to consider the ethical and legal implications:\n",
        "- **Respect Website Terms of Service**: Always check a website’s terms of service to ensure you are not violating any rules.\n",
        "- **Be Polite**: Avoid making too many requests in a short period. Use delays between requests to prevent overloading the server.\n",
        "- **Use an API if Available**: If a website provides an API, use it instead of scraping. APIs are designed for data access and are often more efficient and reliable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_s9AUtxiEZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Concepts\n",
        "\n"
      ],
      "metadata": {
        "id": "qFDH9kGSiLoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HTML Structure\n",
        "\n",
        "Web pages are structured using HTML (HyperText Markup Language). Understanding the basic structure of HTML is crucial for web scraping.\n",
        "\n",
        "Here's a simple example of an HTML structure:\n",
        "\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Welcome to the Sample Page</h1>\n",
        "    <p>This is a sample paragraph.</p>\n",
        "    <div class=\"content\">\n",
        "        <p>More content here.</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "```"
      ],
      "metadata": {
        "id": "b10wjNHoiPX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSS Selectors\n",
        "\n"
      ],
      "metadata": {
        "id": "Vl5wkeMDjJ_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSS (Cascading Style Sheets) selectors are used to select and style HTML elements. They are also useful for selecting elements when scraping.\n",
        "\n",
        "- Element Selector: p selects all <p> elements.\n",
        "- Class Selector: .content selects all elements with class \"content\".\n",
        "- ID Selector: #main selects the element with id \"main\"."
      ],
      "metadata": {
        "id": "m4Z9ZGPgmCpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools and Libraries for Web Scraping\n",
        "\n"
      ],
      "metadata": {
        "id": "v7r2vn_NkLV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### BeautifulSoup\n",
        "\n",
        "BeautifulSoup is a Python library used for parsing HTML and XML documents. It creates a parse tree that can be used to extract data from HTML.\n"
      ],
      "metadata": {
        "id": "qeLhe6rqkUZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Scrapy\n",
        "Scrapy is an open-source and collaborative web crawling framework for Python. It is used to extract data from websites and process them as required.\n",
        "\n"
      ],
      "metadata": {
        "id": "IG8-Z37mkaM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selenium\n",
        "Selenium is a powerful tool for controlling web browsers through programs and performing browser automation. It is useful for scraping dynamic content that requires JavaScript execution."
      ],
      "metadata": {
        "id": "KnqkvF5nkbKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Examples\n"
      ],
      "metadata": {
        "id": "ioY4meVbkk_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple HTML Parsing with BeautifulSoup\n",
        "Let's start with a basic example of using BeautifulSoup to parse HTML and extract data."
      ],
      "metadata": {
        "id": "fKYy6l2Fk2iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Fetch the webpage\n",
        "url = 'https://phet-dev.colorado.edu/html/build-an-atom/0.0.0-3/simple-text-only-test-page.html'\n",
        "# url = 'http://web.simmons.edu/~grovesd/comm244/class/html-testing.html'\n",
        "# url = 'https://github.com/dashboard'\n",
        "# url = 'http://example.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the title of the page\n",
        "title = soup.title.string\n",
        "print('Page Title:', title)\n",
        "\n",
        "# Extract all headers (h1, h2, h3, h4, h5, h6)\n",
        "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "for i, header in enumerate(headers, 1):\n",
        "    print(f'{header.name.upper()} {i}:', header.text)\n",
        "\n",
        "# Extract all paragraphs\n",
        "paragraphs = soup.find_all('p')\n",
        "for i, p in enumerate(paragraphs, 1):\n",
        "    print(f'Paragraph {i}:', p.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2gC9zTFh-Av",
        "outputId": "32872e46-fbe7-49b4-c96e-de6e8b356cf8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Simple Text Only Test Page\n",
            "H1 1: Simple Test Page\n",
            "Paragraph 1: This page tests that simple text can be rendered using the basic HTML5 boiler plate outline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Navigating and Scraping Complex Web Pages"
      ],
      "metadata": {
        "id": "-dg8H2balAQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more complex pages, we can use CSS selectors to find elements."
      ],
      "metadata": {
        "id": "xxcJ08vklCeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract elements with class 'content'\n",
        "content_divs = soup.select('.content')\n",
        "for div in content_divs:\n",
        "    print('Content:', div.text)"
      ],
      "metadata": {
        "id": "kAPeDB_4lEvA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Dynamic Content with Selenium"
      ],
      "metadata": {
        "id": "7lVYBZRClKaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with dynamic content, Selenium can be used to render JavaScript."
      ],
      "metadata": {
        "id": "B970IHrDlNI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n",
        "# Set up the Selenium WebDriver (using Chrome in this example)\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Navigate to the webpage\n",
        "driver.get('http://example.com')\n",
        "\n",
        "# Allow time for JavaScript to execute\n",
        "time.sleep(5)\n",
        "\n",
        "# Extract data\n",
        "dynamic_content = driver.find_element(By.CLASS_NAME, 'dynamic')\n",
        "print('Dynamic Content:', dynamic_content.text)\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "iFkZ1MgOlMZI",
        "outputId": "0af99df7-531e-47ac-88b9-5141a6411ad5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SessionNotCreatedException",
          "evalue": "Message: session not created: Chrome failed to start: exited normally.\n  (session not created: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /root/.cache/selenium/chrome/linux64/125.0.6422.141/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x56f0204cce3a <unknown>\n#1 0x56f0201b645c <unknown>\n#2 0x56f0201eb6f8 <unknown>\n#3 0x56f0201e763b <unknown>\n#4 0x56f020231b19 <unknown>\n#5 0x56f020225253 <unknown>\n#6 0x56f0201f51c7 <unknown>\n#7 0x56f0201f5b3e <unknown>\n#8 0x56f02049327b <unknown>\n#9 0x56f020497327 <unknown>\n#10 0x56f02047fdae <unknown>\n#11 0x56f020497df2 <unknown>\n#12 0x56f02046474f <unknown>\n#13 0x56f0204bc128 <unknown>\n#14 0x56f0204bc2fb <unknown>\n#15 0x56f0204cbf6c <unknown>\n#16 0x7ce47420cac3 <unknown>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a2b3500493c2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Set up the Selenium WebDriver (using Chrome in this example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Navigate to the webpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mvendor_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: Chrome failed to start: exited normally.\n  (session not created: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /root/.cache/selenium/chrome/linux64/125.0.6422.141/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x56f0204cce3a <unknown>\n#1 0x56f0201b645c <unknown>\n#2 0x56f0201eb6f8 <unknown>\n#3 0x56f0201e763b <unknown>\n#4 0x56f020231b19 <unknown>\n#5 0x56f020225253 <unknown>\n#6 0x56f0201f51c7 <unknown>\n#7 0x56f0201f5b3e <unknown>\n#8 0x56f02049327b <unknown>\n#9 0x56f020497327 <unknown>\n#10 0x56f02047fdae <unknown>\n#11 0x56f020497df2 <unknown>\n#12 0x56f02046474f <unknown>\n#13 0x56f0204bc128 <unknown>\n#14 0x56f0204bc2fb <unknown>\n#15 0x56f0204cbf6c <unknown>\n#16 0x7ce47420cac3 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Storage"
      ],
      "metadata": {
        "id": "RHN_AsdKlV2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing and Cleaning Scraped Data\n"
      ],
      "metadata": {
        "id": "c5SXgXInlXXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After extracting data, you may need to clean it for analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "4s2oppgBlZEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of cleaning data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Paragraphs': [p.text for p in paragraphs]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEAU6U45lZ-h",
        "outputId": "2c9b3cb4-eef5-4c3b-d25c-53f86a1ebb49"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          Paragraphs\n",
            "0  This page tests that simple text can be render...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing Data in CSV, JSON, or Databases\n"
      ],
      "metadata": {
        "id": "tlXQI4UXlbNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can store the cleaned data in various formats for later use.\n",
        "\n"
      ],
      "metadata": {
        "id": "EGcQ5R59ltXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "df.to_csv('scraped_data.csv', index=False)\n",
        "\n",
        "# Save to JSON\n",
        "df.to_json('scraped_data.json', orient='records')"
      ],
      "metadata": {
        "id": "xRbwMu_3luUU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these examples and tools, you should be able to start scraping data from web pages. In the next section, we will explore APIs and how to use them to gather data."
      ],
      "metadata": {
        "id": "IK09-IGFlwD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APIs\n",
        "\n"
      ],
      "metadata": {
        "id": "a32KYJR8utHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is an API?\n",
        "\n",
        "An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other. APIs provide a way for developers to interact with an external service using a predefined set of commands.\n",
        "\n"
      ],
      "metadata": {
        "id": "Op7AJo_Ruvay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of APIs\n",
        "\n",
        "- **REST APIs**: Representational State Transfer (REST) APIs use HTTP requests to perform CRUD (Create, Read, Update, Delete) operations. They are stateless and use standard HTTP methods like GET, POST, PUT, DELETE.\n",
        "\n",
        "- **GraphQL APIs**: GraphQL APIs allow clients to request exactly the data they need. Unlike REST, which requires multiple endpoints, GraphQL uses a single endpoint to fetch data.\n",
        "\n"
      ],
      "metadata": {
        "id": "MOKm3MVMuxok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding API Documentation\n",
        "\n",
        "API documentation provides information on how to use the API, including available endpoints, request methods, required parameters, and example responses. Key sections to look for:\n",
        "- **Authentication**: Methods to authenticate requests (API keys, OAuth tokens).\n",
        "- **Endpoints**: URLs for accessing different parts of the API.\n",
        "- **Parameters**: Required and optional parameters for each endpoint.\n",
        "- **Response Format**: Structure of the data returned by the API (usually JSON).\n",
        "\n"
      ],
      "metadata": {
        "id": "eiZ79l4Suzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools and Libraries for API Interaction\n",
        "TODO expand on HTTP Requests (maybe even before this section)\n",
        "- **Requests**: A popular Python library for making HTTP requests. (As we have already seen in Web Scraping)\n",
        "- **Postman**: A tool for testing and interacting with APIs, useful for understanding API behavior and responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "HNc0He36u1IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Examples\n",
        "\n"
      ],
      "metadata": {
        "id": "lUBAoB_au2oE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making Simple API Calls\n",
        "\n",
        "Let's start with a simple example of making a GET request to an API using the Requests library.\n",
        "\n"
      ],
      "metadata": {
        "id": "80NtcDDIu4P9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import requests\n",
        "\n",
        "# Define the API endpoint and parameters\n",
        "url = 'https://api.example.com/data'\n",
        "params = {'key': 'your_api_key', 'query': 'example'}\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "# Check the status code\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON response\n",
        "    data = response.json()\n",
        "    print(data)\n",
        "else:\n",
        "    print(f'Error: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "mItPBewIu5rD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO Add a practical simple example of using API (Spotify or the HTML Dan sent us)"
      ],
      "metadata": {
        "id": "OQVfUHGswVnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling API Responses"
      ],
      "metadata": {
        "id": "w7Q7kakxwcbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "APIs typically return data in JSON format. Here's how to handle and parse JSON responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "QXAGAcuJwgBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming response is the result of requests.get()\n",
        "\n",
        "# Parse the JSON response\n",
        "data = response.json()\n",
        "\n",
        "# Access specific fields\n",
        "item = data['items'][0]\n",
        "name = item['name']\n",
        "print('Name:', name)\n"
      ],
      "metadata": {
        "id": "Rwil8G18whGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pagination and Rate Limiting"
      ],
      "metadata": {
        "id": "3FD1dn7qwpvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many APIs paginate their responses to limit the amount of data returned in a single request. Here's how to handle pagination.\n",
        "\n"
      ],
      "metadata": {
        "id": "uCQ88GZ7wrpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "url = 'https://api.example.com/data'\n",
        "params = {'key': 'your_api_key', 'query': 'example', 'page': 1}\n",
        "\n",
        "# Loop to handle pagination\n",
        "all_data = []\n",
        "while True:\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        break\n",
        "\n",
        "    data = response.json()\n",
        "    all_data.extend(data['items'])\n",
        "\n",
        "    # Check for next page\n",
        "    if 'next_page' in data:\n",
        "        params['page'] += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(f'Total items fetched: {len(all_data)}')\n"
      ],
      "metadata": {
        "id": "QQ97rPDKwuEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining Web Scraping and APIs\n",
        "\n",
        "### Integrating Data from Web Scraping and APIs\n",
        "\n",
        "Combining data from web scraping and APIs can provide a richer and more comprehensive dataset. This approach allows you to fill in gaps where one method alone might fall short. Here, we will discuss how to effectively integrate data from both sources.\n",
        "\n",
        "### Practical Examples\n",
        "\n",
        "#### Example 1: Enhancing Product Data\n",
        "\n",
        "Imagine you are building a product comparison tool. You might scrape product details from an e-commerce website and then use an API to get additional information, such as customer reviews or ratings.\n",
        "\n",
        "**Step 1: Scrape Product Details**\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Scrape product details from an e-commerce site\n",
        "url = 'http://example-ecommerce.com/products'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract product information\n",
        "products = []\n",
        "for item in soup.select('.product-item'):\n",
        "    name = item.select_one('.product-name').text\n",
        "    price = item.select_one('.product-price').text\n",
        "    product_id = item['data-id']\n",
        "    products.append({'product_id': product_id, 'name': name, 'price': price})\n",
        "\n",
        "df_products = pd.DataFrame(products)\n",
        "print(df_products)\n"
      ],
      "metadata": {
        "id": "nOUB2h-HyA7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO The main goal of this section is to only compare between the two (no code needed in my opinion) -\n",
        "\n",
        " https://medium.com/analytics-vidhya/render-html-template-on-google-colab-1532763234e3\n"
      ],
      "metadata": {
        "id": "RKqL8TAayWLN"
      }
    }
  ]
}